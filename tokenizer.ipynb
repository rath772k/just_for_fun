{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/paul_graham_essay.txt\", \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75012"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mq20B5\\nw;[yaGdAV8DRLm!H3g.F9 vIl%c\\'b/$O?\"]k:CNPW6+xo1,ps4nJKift-X)7UuEhSjz—YT&e(r'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some random string\n"
     ]
    }
   ],
   "source": [
    "stoi = {char: idx for idx, char in enumerate(vocab)}\n",
    "itos = {idx: char for char, idx in stoi.items()}\n",
    "\n",
    "def encode(text):\n",
    "    return [stoi[char] for char in text]\n",
    "\n",
    "def decode(tokens):\n",
    "    return \"\".join(itos[token] for token in tokens)\n",
    "\n",
    "test_text = \"some random string\"\n",
    "\n",
    "print(decode(encode(test_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tokenized_data = torch.tensor(encode(data), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(tokenized_data))\n",
    "train_data = tokenized_data[:n]\n",
    "test_data = tokenized_data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=vocab_size)\n",
    "    \n",
    "    def forward(self, inputs, targets=None):\n",
    "        # inputs.shape -> (batch_size, context_length)\n",
    "        logits =  self.embedding(inputs)\n",
    "        # logits.shape -> (batch_size, context_length, vocab_size)\n",
    "        # but for torch's cross entropy loss, need (batch_size, vocab_size, context_length)\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "        logits = logits.permute(0, 2, 1)\n",
    "        loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        logits = logits.permute(0, 2, 1)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, start_idx, max_tokens=100):\n",
    "        for _ in range(max_tokens):\n",
    "            logits, _ = self(start_idx)\n",
    "            # we only care about last prediction\n",
    "            pred = logits[:, -1, :] # (batch_size, vocab_size)\n",
    "            prob = torch.nn.functional.softmax(pred, dim=-1)\n",
    "            idx = prob.multinomial(num_samples=1) # (batch_size, 1)\n",
    "            start_idx = torch.cat([start_idx, idx], dim=-1)\n",
    "        return start_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramModel(\n",
       "  (embedding): Embedding(81, 81)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = BigramModel(len(vocab))\n",
    "m.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 16\n",
    "batch_size = 16\n",
    "unif = torch.ones(train_data.shape[0] - context_length)\n",
    "unif.to(device)\n",
    "def get_batch():\n",
    "    batch_indices = unif.multinomial(batch_size, replacement=False)\n",
    "    inputs = torch.stack([train_data[i:i+context_length] for i in batch_indices])\n",
    "    targets = torch.stack([train_data[i+1:i+1+context_length] for i in batch_indices])\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    return inputs, targets\n",
    "\n",
    "logits, loss = m(*get_batch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 81])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.0071, device='cuda:0', grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[51, 28,  7, 51, 80, 13, 55, 28, 13, 60, 13, 57, 34, 62, 28, 24],\n",
      "        [60, 57, 25, 28, 47, 78, 28, 11, 31, 55, 51, 28, 57, 51, 62, 60],\n",
      "        [55, 54, 25, 28, 30, 28, 42, 57, 78,  7, 28, 61, 80, 51, 20, 28],\n",
      "        [ 6, 76, 70, 78, 28, 24, 51, 51, 13, 28, 54, 11, 80, 62, 28,  7]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[51, 28,  7, 51, 80, 13, 55, 28, 13, 60, 13, 57, 34, 62, 28, 24, 24],\n",
       "        [60, 57, 25, 28, 47, 78, 28, 11, 31, 55, 51, 28, 57, 51, 62, 60, 51],\n",
       "        [55, 54, 25, 28, 30, 28, 42, 57, 78,  7, 28, 61, 80, 51, 20, 28, 27],\n",
       "        [ 6, 76, 70, 78, 28, 24, 51, 51, 13, 28, 54, 11, 80, 62, 28,  7, 76]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx = get_batch()[0]\n",
    "print(start_idx)\n",
    "m.generate(start_idx, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o words didn\\'t ga9DngiAaz&:Dtt[&851y)95sk7?V5o+0o.If P+K??4/HXAyO?(9\"-lD(IWW3—\\'sPfa8f8\\'3.zT\"n?\"Wu]\\n+)&$\"W981o/gGVM7c'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(m.generate(start_idx, 100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(params=m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3222, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10000):\n",
    "    _, loss = m(*get_batch())\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"o words didn't gobyor line abud d noft d ee, ors, o.\\nI op Dar tzeon othanid Th mes oulvin guthis), d is. des soubour\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(m.generate(start_idx, 100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tokenizers import BPETokenizer\n",
    "import torch\n",
    "from transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "\n",
    "TOKENIZER_PATH = f\"tokenizers/bpe_{VOCAB_SIZE}_tokenizer.pkl\"\n",
    "DATA_PATH = \"../data/paul_graham_essay.txt\"\n",
    "TRAIN_DATA_PATH = f\"bpe_{VOCAB_SIZE}_train.pt\"\n",
    "TEST_DATA_PATH = f\"bpe_{VOCAB_SIZE}_test.pt\"\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "EMBED_SIZE = 384\n",
    "NUM_HEADS = 8\n",
    "CONTEXT_LENGTH = 16\n",
    "NUM_LAYERS = 6\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(TOKENIZER_PATH, \"rb\") as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "except:\n",
    "    tokenizer = BPETokenizer.build_tokenizer(DATA_PATH, VOCAB_SIZE)\n",
    "    with open(TOKENIZER_PATH, \"wb\") as f:\n",
    "        pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_data = torch.load(TRAIN_DATA_PATH)\n",
    "    test_data = torch.load(TEST_DATA_PATH)\n",
    "except:\n",
    "    with open(DATA_PATH, \"r\") as f:\n",
    "        data = f.read()\n",
    "    tokenized_data = torch.tensor(tokenizer.encode(data), dtype=torch.long)\n",
    "    n = int(0.9 * len(tokenized_data))\n",
    "    train_data = tokenized_data[:n]\n",
    "    test_data = tokenized_data[n:]\n",
    "    torch.save(train_data, TRAIN_DATA_PATH)\n",
    "    torch.save(test_data, TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (token_embedding): Embedding(1000, 384)\n",
       "  (pos_embedding): Embedding(16, 384)\n",
       "  (attn_blocks): ModuleList(\n",
       "    (0-5): 6 x AttentionBlock(\n",
       "      (attn_heads): ModuleList(\n",
       "        (0-7): 8 x CausalSelfAttention(\n",
       "          (Q): Linear(in_features=384, out_features=48, bias=False)\n",
       "          (K): Linear(in_features=384, out_features=48, bias=False)\n",
       "          (V): Linear(in_features=384, out_features=48, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (fcn): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (activation): ReLU()\n",
       "        (proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=384, out_features=1000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Transformer(vocab_size=VOCAB_SIZE, embed_size=EMBED_SIZE, num_heads=NUM_HEADS, context_length=CONTEXT_LENGTH, num_layers=NUM_LAYERS)\n",
    "m.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unif = torch.ones(train_data.shape[0] - CONTEXT_LENGTH, device=DEVICE)\n",
    "def get_batch():\n",
    "    batch_indices = unif.multinomial(BATCH_SIZE, replacement=False)\n",
    "    inputs = torch.stack([train_data[i:i+CONTEXT_LENGTH] for i in batch_indices])\n",
    "    targets = torch.stack([train_data[i+1:i+1+CONTEXT_LENGTH] for i in batch_indices])\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    targets = targets.to(DEVICE)\n",
    "    return inputs, targets\n",
    "\n",
    "logits, loss = m(*get_batch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 16, 1000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0593485832214355"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[411, 659, 269, 101, 321,  79, 486, 274, 835, 314, 376, 377, 690, 527,\n",
      "         658, 422],\n",
      "        [669, 280, 707, 858, 304, 257, 529, 327, 364, 790, 271, 321,  79, 336,\n",
      "          32, 116],\n",
      "        [257, 564, 413, 286, 268, 103, 696, 122, 256, 533, 270, 694, 288, 278,\n",
      "         421, 286],\n",
      "        [315, 991, 110, 267, 294, 286, 116, 309, 647, 466, 597, 552, 421, 560,\n",
      "         354, 624],\n",
      "        [674, 705, 345, 529, 280, 102, 394, 332, 116, 272, 271, 298, 101, 102,\n",
      "         495, 766],\n",
      "        [449, 630, 683, 464, 345, 283, 269,  99, 353, 574, 112, 553, 381, 589,\n",
      "         816, 287],\n",
      "        [574, 707, 112, 553, 287, 296, 900, 363, 670, 104, 117, 559, 489, 416,\n",
      "         116, 631],\n",
      "        [794, 280, 118, 379, 591, 393, 795, 476, 278, 885, 424, 739, 451, 680,\n",
      "         103, 389],\n",
      "        [109, 788, 299, 343, 114,  32, 641, 294, 102, 326, 116, 322, 116, 311,\n",
      "         360, 714],\n",
      "        [330,  49,  48,  37,  41,  32, 288, 476, 694, 571, 670,  77,  73,  84,\n",
      "          32, 738],\n",
      "        [973, 548, 110, 887, 693, 273, 448,  85, 110, 105, 120,  32, 327, 514,\n",
      "         557, 381],\n",
      "        [582, 262, 471, 485, 278, 293, 447, 273, 274, 110,  97, 361, 275,  72,\n",
      "         379, 107],\n",
      "        [298, 261, 365, 646, 100, 386, 270, 822, 119, 266, 728, 424, 518, 370,\n",
      "         419, 301],\n",
      "        [397,  97, 261, 265, 527, 658, 518, 349, 342, 387, 116, 358, 334, 260,\n",
      "         594, 339],\n",
      "        [944, 518, 349, 342, 285, 472, 269, 347, 111, 486, 274, 835, 740, 995,\n",
      "         115, 439],\n",
      "        [258, 114, 497, 280, 104, 101, 440, 270, 822, 869, 275, 329, 112, 297,\n",
      "         406, 122],\n",
      "        [101, 121, 720, 274, 942, 805, 261, 365, 646, 100, 386,  45, 817, 100,\n",
      "         273, 494],\n",
      "        [100, 261, 110, 290, 649, 749, 277, 294, 400, 616, 116, 260, 299, 698,\n",
      "         275, 803],\n",
      "        [290, 258, 795, 428, 394, 293, 265, 301, 908, 829, 294, 713,  51, 369,\n",
      "         116, 368],\n",
      "        [282, 821, 104, 684, 275, 774, 261, 256, 322, 119, 360, 428, 349, 105,\n",
      "         639, 256],\n",
      "        [103, 286, 103, 621, 853, 766, 299, 285, 297, 784, 260, 278, 952, 397,\n",
      "         357, 581],\n",
      "        [ 72, 268, 394, 735, 273, 610, 356, 647,  99, 429, 297, 892, 399, 261,\n",
      "         914, 120],\n",
      "        [817, 292, 451, 468, 389, 285, 817, 256, 442, 110,  39, 116, 270,  79,\n",
      "         371, 280],\n",
      "        [314, 101, 271, 747, 953, 121, 769, 695, 451, 381, 276, 962, 434, 744,\n",
      "         536, 115],\n",
      "        [392, 978, 307, 436, 296, 278, 648, 728, 744, 325, 675, 774, 347, 330,\n",
      "         500, 360],\n",
      "        [275, 399, 110,  97, 105, 340, 946,  32, 998, 556, 102, 265, 529, 274,\n",
      "         635, 267],\n",
      "        [296, 851, 256, 677, 757, 280, 109, 276, 103, 261, 377, 109, 378, 105,\n",
      "         117, 413],\n",
      "        [472, 618, 256, 878, 292, 301, 330, 679, 347, 821, 294, 741, 265, 285,\n",
      "          99, 443],\n",
      "        [275, 522, 267, 275,  99, 326, 335, 477, 612, 115, 268, 429, 423, 279,\n",
      "         261, 766],\n",
      "        [745, 289, 277,  67, 520, 606, 110, 979, 330, 513, 863, 101, 119, 419,\n",
      "         694,  77],\n",
      "        [110, 268, 972, 814, 294, 603, 864, 118, 268, 887, 339, 714, 923, 498,\n",
      "         529, 263],\n",
      "        [273, 307, 277, 896, 121, 368, 268, 449, 261, 377,  97, 120, 105, 291,\n",
      "         292, 873]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[411, 659, 269, 101, 321,  79, 486, 274, 835, 314, 376, 377, 690, 527,\n",
       "         658, 422, 127],\n",
       "        [669, 280, 707, 858, 304, 257, 529, 327, 364, 790, 271, 321,  79, 336,\n",
       "          32, 116, 850],\n",
       "        [257, 564, 413, 286, 268, 103, 696, 122, 256, 533, 270, 694, 288, 278,\n",
       "         421, 286, 141],\n",
       "        [315, 991, 110, 267, 294, 286, 116, 309, 647, 466, 597, 552, 421, 560,\n",
       "         354, 624, 556],\n",
       "        [674, 705, 345, 529, 280, 102, 394, 332, 116, 272, 271, 298, 101, 102,\n",
       "         495, 766, 324],\n",
       "        [449, 630, 683, 464, 345, 283, 269,  99, 353, 574, 112, 553, 381, 589,\n",
       "         816, 287, 241],\n",
       "        [574, 707, 112, 553, 287, 296, 900, 363, 670, 104, 117, 559, 489, 416,\n",
       "         116, 631, 701],\n",
       "        [794, 280, 118, 379, 591, 393, 795, 476, 278, 885, 424, 739, 451, 680,\n",
       "         103, 389, 325],\n",
       "        [109, 788, 299, 343, 114,  32, 641, 294, 102, 326, 116, 322, 116, 311,\n",
       "         360, 714,   7],\n",
       "        [330,  49,  48,  37,  41,  32, 288, 476, 694, 571, 670,  77,  73,  84,\n",
       "          32, 738, 538],\n",
       "        [973, 548, 110, 887, 693, 273, 448,  85, 110, 105, 120,  32, 327, 514,\n",
       "         557, 381, 743],\n",
       "        [582, 262, 471, 485, 278, 293, 447, 273, 274, 110,  97, 361, 275,  72,\n",
       "         379, 107, 202],\n",
       "        [298, 261, 365, 646, 100, 386, 270, 822, 119, 266, 728, 424, 518, 370,\n",
       "         419, 301, 884],\n",
       "        [397,  97, 261, 265, 527, 658, 518, 349, 342, 387, 116, 358, 334, 260,\n",
       "         594, 339, 561],\n",
       "        [944, 518, 349, 342, 285, 472, 269, 347, 111, 486, 274, 835, 740, 995,\n",
       "         115, 439, 799],\n",
       "        [258, 114, 497, 280, 104, 101, 440, 270, 822, 869, 275, 329, 112, 297,\n",
       "         406, 122, 387],\n",
       "        [101, 121, 720, 274, 942, 805, 261, 365, 646, 100, 386,  45, 817, 100,\n",
       "         273, 494,   9],\n",
       "        [100, 261, 110, 290, 649, 749, 277, 294, 400, 616, 116, 260, 299, 698,\n",
       "         275, 803, 648],\n",
       "        [290, 258, 795, 428, 394, 293, 265, 301, 908, 829, 294, 713,  51, 369,\n",
       "         116, 368, 148],\n",
       "        [282, 821, 104, 684, 275, 774, 261, 256, 322, 119, 360, 428, 349, 105,\n",
       "         639, 256, 189],\n",
       "        [103, 286, 103, 621, 853, 766, 299, 285, 297, 784, 260, 278, 952, 397,\n",
       "         357, 581, 758],\n",
       "        [ 72, 268, 394, 735, 273, 610, 356, 647,  99, 429, 297, 892, 399, 261,\n",
       "         914, 120, 402],\n",
       "        [817, 292, 451, 468, 389, 285, 817, 256, 442, 110,  39, 116, 270,  79,\n",
       "         371, 280, 864],\n",
       "        [314, 101, 271, 747, 953, 121, 769, 695, 451, 381, 276, 962, 434, 744,\n",
       "         536, 115, 155],\n",
       "        [392, 978, 307, 436, 296, 278, 648, 728, 744, 325, 675, 774, 347, 330,\n",
       "         500, 360, 649],\n",
       "        [275, 399, 110,  97, 105, 340, 946,  32, 998, 556, 102, 265, 529, 274,\n",
       "         635, 267, 963],\n",
       "        [296, 851, 256, 677, 757, 280, 109, 276, 103, 261, 377, 109, 378, 105,\n",
       "         117, 413, 285],\n",
       "        [472, 618, 256, 878, 292, 301, 330, 679, 347, 821, 294, 741, 265, 285,\n",
       "          99, 443, 123],\n",
       "        [275, 522, 267, 275,  99, 326, 335, 477, 612, 115, 268, 429, 423, 279,\n",
       "         261, 766, 834],\n",
       "        [745, 289, 277,  67, 520, 606, 110, 979, 330, 513, 863, 101, 119, 419,\n",
       "         694,  77,  79],\n",
       "        [110, 268, 972, 814, 294, 603, 864, 118, 268, 887, 339, 714, 923, 498,\n",
       "         529, 263, 137],\n",
       "        [273, 307, 277, 896, 121, 368, 268, 449, 261, 377,  97, 120, 105, 291,\n",
       "         292, 873, 334]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx = get_batch()[0]\n",
    "print(start_idx)\n",
    "m.generate(start_idx, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was aViabecaed �used ��I could rowyou could isiclearust somit was timenext reader omthing�I'd ore generdid trying to : writ�used stordays used pept who[U&Lisp ffirmake go\u0010what �ig magut onan when Fconfhow turdone publiubli�I at summa lot of \u0004�ment are becomwas perap�typvery ��imag�pept Xen't �\u0003\u000e�generunderSstuamge: good \n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(m.generate(torch.tensor([tokenizer.encode(\"I was a\")], device=DEVICE), 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(params=m.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.06289529800415\n",
      "6.192881107330322\n",
      "5.641086101531982\n",
      "4.955998420715332\n",
      "4.46861457824707\n",
      "4.0264787673950195\n",
      "3.936904191970825\n",
      "3.50254225730896\n",
      "3.1594178676605225\n",
      "2.8628921508789062\n",
      "I was a. I kept went paint them placantagented partly ul to se igious 1write 222, we formout of could ss could , because it was bworked in N that didn't strimmeded to do it started to have been the day in publishing till t, 2010, in phBe stomatural out of capary to buhknew how when art discofor so stack of bigiousser, to be ? \n"
     ]
    }
   ],
   "source": [
    "for step in range(1000):\n",
    "    _, loss = m(*get_batch())\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if step % 100 == 0:\n",
    "        print(loss.item())\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(m.generate(torch.tensor([tokenizer.encode(\"I was a\")], device=DEVICE), 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.498555898666382\n",
      "2.3501780033111572\n",
      "2.036022424697876\n",
      "1.6520980596542358\n",
      "1.5975638628005981\n",
      "1.3270169496536255\n",
      "1.0278438329696655\n",
      "0.9670493602752686\n",
      "0.7849155068397522\n",
      "0.7880560159683228\n",
      "I was avertes for twas out? I had no Bel to work talk. But axphotographilally infortun3 involnot, but a po, and partly because it would be a language photits centation and adds who wanted one of online ight land liviv, from revil, and of it.\n",
      "\n",
      "The good per for programs were ting on the flong why to use and inition. \n"
     ]
    }
   ],
   "source": [
    "#Let's do a 1000 more steps and see\n",
    "for step in range(1000):\n",
    "    _, loss = m(*get_batch())\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if step % 100 == 0:\n",
    "        print(loss.item())\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(m.generate(torch.tensor([tokenizer.encode(\"I was a\")], device=DEVICE), 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6662266254425049\n",
      "0.6020451188087463\n",
      "0.49546894431114197\n",
      "0.48122438788414\n",
      "0.5078184604644775\n",
      "0.5251424312591553\n",
      "0.4856676459312439\n",
      "0.455766886472702\n",
      "0.45577189326286316\n",
      "0.4544914662837982\n",
      "I was aight go back to RISD, but fund a bunch of startups all onced at the prospect of having to stand up in front of a group of people and tell them something that won't waste their time is a greal skey rengage in to make this work. By means of an egregious collection of hacks I managed to ved at Corne\n"
     ]
    }
   ],
   "source": [
    "#1000 more?\n",
    "for step in range(1000):\n",
    "    _, loss = m(*get_batch())\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if step % 100 == 0:\n",
    "        print(loss.item())\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(m.generate(torch.tensor([tokenizer.encode(\"I was a\")], device=DEVICE), 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I wanted not just to build things. I had plenty of respect for theory — indeed, a snea-d of Microsoft or Goldman Sachs.\n",
      "\n",
      "The deal for startups was based on a combination and adds it or YC GDLU, partly because if you underst10% sure it's even a good way to paint. But it seemed a good enough bet to be worth trying.\n",
      "\n",
      "Our \n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(m.generate(torch.tensor([tokenizer.encode(\"I wanted not just to build things\")], device=DEVICE), 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
